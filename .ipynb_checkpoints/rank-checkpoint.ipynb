{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7d86ce",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbb2d2",
   "metadata": {},
   "source": [
    "Second Assignement of Learning with Massive Data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26dd9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "path_name_documents = './Databases/prova/gigi.jsonl'\n",
    "path_name_query = './Databases/prova/query.jsonl'\n",
    "\n",
    "\n",
    "def readFile(path_name):\n",
    "    # Load the JSONL file into a list\n",
    "    with open(path_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Convert each JSON object into a dictionary\n",
    "    dicts = [json.loads(line) for line in lines]\n",
    "\n",
    "    # Convert the dictionaries into arrays and stack them vertically\n",
    "    arrays = np.vstack([np.array(list(d.values())) for d in dicts])\n",
    "\n",
    "    # Convert the arrays into a list of lists\n",
    "    text = arrays.tolist()\n",
    "    \n",
    "    return text\n",
    "\n",
    "documents = readFile(path_name_documents)\n",
    "#print(documents)\n",
    "query = readFile(path_name_query)\n",
    "#print(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade98943",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2820a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stemmingLemming(filtered_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Perform stemming or lemmatization on filtered tokens\n",
    "    \n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return filtered_tokens\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "def tokenize(path_name):\n",
    "    \n",
    "    with open(path_name, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "        # Create an empty list to store the tokenized documents\n",
    "        tokenized_docs = []\n",
    "\n",
    "        # Loop through each line in the JSONL file\n",
    "        for line in data:\n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            doc = json.loads(line)\n",
    "\n",
    "            # Extract the text from the dictionary\n",
    "            text = doc['text']\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            #text = re.sub(r'\\d+', '', text)  # Remove all numbers\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove all punctuation\n",
    "\n",
    "            # Tokenize the text using NLTK\n",
    "            tokens = word_tokenize(text)\n",
    "            tokensStemLem = stemmingLemming(tokens)\n",
    "\n",
    "            # Add the tokenized document to the list\n",
    "            tokenized_docs.append(tokensStemLem)\n",
    "\n",
    "        # Print the tokenized documents\n",
    "    return tokenized_docs\n",
    "\n",
    "tokenized_docs = tokenize(path_name_documents)\n",
    "\n",
    "\n",
    "tokenized_query = tokenize(path_name_query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72f5c9",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8650446d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'in': 0.05852609374515299, 'ho': 0.05215886798668843, 'of': 0.05215886798668843, 'and': 0.045758344661276214, 'to': 0.045758344661276214, 'the': 0.045758344661276214, 'a': 0.032856355034394685, 'co': 0.032856355034394685, 'heme': 0.026354356675486374, 'it': 0.026354356675486374, 'play': 0.01981799656161261, 'student': 0.01324700157864003, 'oxygenas': 0.01324700157864003, 'an': 0.01324700157864003, 'stress': 0.01324700157864003, 'role': 0.01324700157864003, 'by': 0.01324700157864003, 'antiinflammatori': 0.01324700157864003, 'effect': 0.01324700157864003, 'for': 0.01324700157864003, 'with': 0.01324700157864003, 'pulmonari': 0.01324700157864003, 'stude': 0.006641095710391907, 'studi': 0.006641095710391907, 'induc': 0.006641095710391907, 'protein': 0.006641095710391907, 'confer': 0.006641095710391907, 'cytoprotect': 0.006641095710391907, 'against': 0.006641095710391907, 'oxid': 0.006641095710391907, 'vitro': 0.006641095710391907, 'vivo': 0.006641095710391907, 'addit': 0.006641095710391907, 'physiolog': 0.006641095710391907, 'degrad': 0.006641095710391907, 'may': 0.006641095710391907, 'influenc': 0.006641095710391907, 'number': 0.006641095710391907, 'cellular': 0.006641095710391907, 'process': 0.006641095710391907, 'includ': 0.006641095710391907, 'growth': 0.006641095710391907, 'inflamm': 0.006641095710391907, 'apoptosi': 0.006641095710391907, 'virtu': 0.006641095710391907, 'limit': 0.006641095710391907, 'tissu': 0.006641095710391907, 'damag': 0.006641095710391907, 'respons': 0.006641095710391907, 'proinflammatori': 0.006641095710391907, 'stimulu': 0.006641095710391907, 'prevent': 0.006641095710391907, 'allograft': 0.006641095710391907, 'reject': 0.006641095710391907, 'after': 0.006641095710391907, 'transplant': 0.006641095710391907, 'transcript': 0.006641095710391907, 'upregul': 0.006641095710391907, 'respond': 0.006641095710391907, 'mani': 0.006641095710391907, 'agent': 0.006641095710391907, 'such': 0.006641095710391907, 'hypoxia': 0.006641095710391907, 'bacteri': 0.006641095710391907, 'lipopolysaccharid': 0.006641095710391907, 'reactiv': 0.006641095710391907, 'oxygennitrogen': 0.006641095710391907, 'speci': 0.006641095710391907, 'constitut': 0.006641095710391907, 'express': 0.006641095710391907, 'isozym': 0.006641095710391907, 'catalyz': 0.006641095710391907, 'ratelimit': 0.006641095710391907, 'step': 0.006641095710391907, 'convers': 0.006641095710391907, 'metabolit': 0.006641095710391907, 'bilirubin': 0.006641095710391907, 'ixÎ±': 0.006641095710391907, 'ferrou': 0.006641095710391907, 'iron': 0.006641095710391907, 'carbon': 0.006641095710391907, 'monoxid': 0.006641095710391907, 'mechan': 0.006641095710391907, 'which': 0.006641095710391907, 'provid': 0.006641095710391907, 'protect': 0.006641095710391907, 'most': 0.006641095710391907, 'like': 0.006641095710391907, 'involv': 0.006641095710391907, 'enzymat': 0.006641095710391907, 'reaction': 0.006641095710391907, 'product': 0.006641095710391907, 'remark': 0.006641095710391907, 'administr': 0.006641095710391907, 'at': 0.006641095710391907, 'low': 0.006641095710391907, 'concentr': 0.006641095710391907, 'can': 0.006641095710391907, 'substitut': 0.006641095710391907, 'respect': 0.006641095710391907, 'antiapoptot': 0.006641095710391907, 'suggest': 0.006641095710391907, 'key': 0.006641095710391907, 'mediat': 0.006641095710391907, 'function': 0.006641095710391907, 'chronic': 0.006641095710391907, 'lowlevel': 0.006641095710391907, 'exogen': 0.006641095710391907, 'exposur': 0.006641095710391907, 'from': 0.006641095710391907, 'cigarett': 0.006641095710391907, 'smoke': 0.006641095710391907, 'contribut': 0.006641095710391907, 'import': 0.006641095710391907, 'medicin': 0.006641095710391907, 'implic': 0.006641095710391907, 'hoco': 0.006641095710391907, 'system': 0.006641095710391907, 'diseas': 0.006641095710391907, 'will': 0.006641095710391907, 'be': 0.006641095710391907, 'discuss': 0.006641095710391907, 'thi': 0.006641095710391907, 'review': 0.006641095710391907, 'emphasi': 0.006641095710391907, 'on': 0.006641095710391907, 'inflammatori': 0.006641095710391907, 'state': 0.006641095710391907}), Counter({'of': 0.17229682716392983, 'the': 0.1491514704332244, 'and': 0.10145187309553684, 'intens': 0.05177650115262176, 'care': 0.05177650115262176, 'medicin': 0.05177650115262176, 'wa': 0.05177650115262176, 'by': 0.05177650115262176, 'in': 0.05177650115262176, 'were': 0.05177650115262176, 'a': 0.05177650115262176, 'second': 0.026159136286326996, 'document': 0.026159136286326996, 'documnent': 0.026159136286326996, 'mart': 0.026159136286326996, 'st': 0.026159136286326996, 'intern': 0.026159136286326996, 'symposium': 0.026159136286326996, 'on': 0.026159136286326996, 'emerg': 0.026159136286326996, 'domin': 0.026159136286326996, 'result': 0.026159136286326996, 'recent': 0.026159136286326996, 'clinic': 0.026159136286326996, 'trial': 0.026159136286326996, 'sepsi': 0.026159136286326996, 'acut': 0.026159136286326996, 'respiratori': 0.026159136286326996, 'distress': 0.026159136286326996, 'syndrom': 0.026159136286326996, 'ard': 0.026159136286326996, 'promis': 0.026159136286326996, 'extracorpor': 0.026159136286326996, 'liver': 0.026159136286326996, 'replac': 0.026159136286326996, 'therapi': 0.026159136286326996, 'noninvas': 0.026159136286326996, 'ventil': 0.026159136286326996, 'other': 0.026159136286326996, 'area': 0.026159136286326996, 'interest': 0.026159136286326996, 'ethic': 0.026159136286326996, 'issu': 0.026159136286326996, 'also': 0.026159136286326996, 'receiv': 0.026159136286326996, 'attent': 0.026159136286326996, 'overal': 0.026159136286326996, 'state': 0.026159136286326996, 'art': 0.026159136286326996, 'lectur': 0.026159136286326996, 'procon': 0.026159136286326996, 'debat': 0.026159136286326996, 'seminar': 0.026159136286326996, 'tutori': 0.026159136286326996, 'high': 0.026159136286326996, 'standard': 0.026159136286326996, 'meet': 0.026159136286326996, 'mark': 0.026159136286326996, 'sens': 0.026159136286326996, 'renew': 0.026159136286326996, 'enthusiasm': 0.026159136286326996, 'that': 0.026159136286326996, 'posit': 0.026159136286326996, 'progress': 0.026159136286326996, 'is': 0.026159136286326996, 'occur': 0.026159136286326996})]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "\n",
    "def calculateTF(tokenized_docs):\n",
    "    term_freqs = []\n",
    "    for doc in tokenized_docs:\n",
    "        doc_freq = collections.Counter(doc) #number of repetition for each word\n",
    "       \n",
    "        total_terms = len(doc) #length for each document\n",
    "        \n",
    "        for term in doc_freq:\n",
    "            doc_freq[term] /= float(total_terms)\n",
    "        term_freqs.append(doc_freq)\n",
    "\n",
    "    return term_freqs\n",
    "\n",
    "\n",
    "\n",
    "def calculateTF2(tokenized_docs):\n",
    "    term_freqs = []\n",
    "    for doc in tokenized_docs:\n",
    "        doc_freq = {}\n",
    "        total_terms = len(doc)\n",
    "        for term in doc:\n",
    "            doc_freq[term] = doc_freq.get(term, 0) + 1\n",
    "        for term in doc_freq:\n",
    "            doc_freq[term] /= float(total_terms)\n",
    "        term_freqs.append(doc_freq)\n",
    "\n",
    "    return term_freqs\n",
    "\n",
    "\n",
    "\n",
    "def calculateTFbm25(tokenized_docs):\n",
    "    term_freqs = []\n",
    "    k1 = 1.5 # parameter for controlling term frequency normalization\n",
    "    b = 0.75 # parameter for controlling document length normalization\n",
    "    avgdl = sum(len(doc) for doc in tokenized_docs) / len(tokenized_docs) # average document length\n",
    "\n",
    "    for doc in tokenized_docs:\n",
    "        doc_freq = collections.Counter(doc) #number of repetition for each word\n",
    "        total_terms = len(doc) #length for each document\n",
    "        doc_len_norm = ((1 - b) + b * (total_terms / avgdl)) # document length normalization factor\n",
    "\n",
    "        for term in doc_freq:\n",
    "            tf = doc_freq[term] / total_terms # term frequency\n",
    "            tf_norm = ((k1 + 1) * tf) / (k1 * ((1 - b) + b * (total_terms / avgdl)) + tf) # normalized term frequency with BM25 weighting\n",
    "            doc_freq[term] = tf_norm\n",
    "\n",
    "        term_freqs.append(doc_freq)\n",
    "\n",
    "    return term_freqs\n",
    "\n",
    "    \n",
    "#term_freqs = calculateTF2(tokenized_docs)\n",
    "term_freqs = calculateTFbm25(tokenized_docs)\n",
    "\n",
    "print(term_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76afbcbc",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "504063d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'play': -0.6931471805599453, 'student': -0.40546510810816444, 'stude': 0.0, 'studi': 0.0, 'heme': -0.916290731874155, 'oxygenase1': 0.0, 'ho1': -1.5040773967762742, 'an': -0.40546510810816444, 'induc': 0.0, 'stress': -0.40546510810816444, 'protein': 0.0, 'confer': 0.0, 'cytoprotect': 0.0, 'against': 0.0, 'oxid': 0.0, 'in': -1.791759469228055, 'vitro': 0.0, 'and': -1.791759469228055, 'vivo': 0.0, 'addit': 0.0, 'to': -1.3862943611198906, 'it': -0.916290731874155, 'physiolog': 0.0, 'role': -0.40546510810816444, 'degrad': 0.0, 'may': 0.0, 'influenc': 0.0, 'a': -1.3862943611198906, 'number': 0.0, 'of': -2.0794415416798357, 'cellular': 0.0, 'process': 0.0, 'includ': 0.0, 'growth': 0.0, 'inflamm': 0.0, 'apoptosi': 0.0, 'by': -0.916290731874155, 'virtu': 0.0, 'antiinflammatori': -0.40546510810816444, 'effect': -0.40546510810816444, 'limit': 0.0, 'tissu': 0.0, 'damag': 0.0, 'respons': 0.0, 'proinflammatori': 0.0, 'stimulu': 0.0, 'prevent': 0.0, 'allograft': 0.0, 'reject': 0.0, 'after': 0.0, 'transplant': 0.0, 'the': -1.9459101490553135, 'transcript': 0.0, 'upregul': 0.0, 'respond': 0.0, 'mani': 0.0, 'agent': 0.0, 'such': 0.0, 'hypoxia': 0.0, 'bacteri': 0.0, 'lipopolysaccharid': 0.0, 'reactiv': 0.0, 'oxygennitrogen': 0.0, 'speci': 0.0, 'constitut': 0.0, 'express': 0.0, 'isozym': 0.0, 'oxygenase2': 0.0, 'catalyz': 0.0, 'ratelimit': 0.0, 'step': 0.0, 'convers': 0.0, 'metabolit': 0.0, 'bilirubin': 0.0, 'ixÎ±': 0.0, 'ferrou': 0.0, 'iron': 0.0, 'carbon': 0.0, 'monoxid': 0.0, 'co': -1.0986122886681098, 'mechan': 0.0, 'which': 0.0, 'provid': 0.0, 'protect': 0.0, 'most': 0.0, 'like': 0.0, 'involv': 0.0, 'enzymat': 0.0, 'reaction': 0.0, 'product': 0.0, 'remark': 0.0, 'administr': 0.0, 'at': 0.0, 'low': 0.0, 'concentr': 0.0, 'can': 0.0, 'substitut': 0.0, 'for': -0.40546510810816444, 'with': -0.40546510810816444, 'respect': 0.0, 'antiapoptot': 0.0, 'suggest': 0.0, 'key': 0.0, 'mediat': 0.0, 'function': 0.0, 'chronic': 0.0, 'lowlevel': 0.0, 'exogen': 0.0, 'exposur': 0.0, 'from': 0.0, 'cigarett': 0.0, 'smoke': 0.0, 'contribut': 0.0, 'import': 0.0, 'pulmonari': -0.40546510810816444, 'medicin': -0.6931471805599453, 'implic': 0.0, 'ho1co': 0.0, 'system': 0.0, 'diseas': 0.0, 'will': 0.0, 'be': 0.0, 'discuss': 0.0, 'thi': 0.0, 'review': 0.0, 'emphasi': 0.0, 'on': -0.40546510810816444, 'inflammatori': 0.0, 'state': -0.40546510810816444, 'second': 0.0, 'document': 0.0, 'documnent': 0.0, '1978': 0.0, 'mart': 0.0, '21st': 0.0, 'intern': 0.0, 'symposium': 0.0, 'intens': -0.40546510810816444, 'care': -0.40546510810816444, 'emerg': 0.0, 'wa': -0.40546510810816444, 'domin': 0.0, 'result': 0.0, 'recent': 0.0, 'clinic': 0.0, 'trial': 0.0, 'sepsi': 0.0, 'acut': 0.0, 'respiratori': 0.0, 'distress': 0.0, 'syndrom': 0.0, 'ard': 0.0, 'promis': 0.0, 'extracorpor': 0.0, 'liver': 0.0, 'replac': 0.0, 'therapi': 0.0, 'noninvas': 0.0, 'ventil': 0.0, 'were': -0.40546510810816444, 'other': 0.0, 'area': 0.0, 'interest': 0.0, 'ethic': 0.0, 'issu': 0.0, 'also': 0.0, 'receiv': 0.0, 'attent': 0.0, 'overal': 0.0, 'art': 0.0, 'lectur': 0.0, 'procon': 0.0, 'debat': 0.0, 'seminar': 0.0, 'tutori': 0.0, 'high': 0.0, 'standard': 0.0, 'meet': 0.0, 'mark': 0.0, 'sens': 0.0, 'renew': 0.0, 'enthusiasm': 0.0, 'that': 0.0, 'posit': 0.0, 'progress': 0.0, 'is': 0.0, 'occur': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculateIDF(tokenized_docs,term_freqs):\n",
    "    \n",
    "    #dictionary with the frequency of each term in all documents\n",
    "    all_terms = [term for doc in tokenized_docs for term in doc]\n",
    "    \n",
    "    df = collections.Counter(all_terms)\n",
    "\n",
    "    # Calculate IDF for each term\n",
    "    N = len(tokenized_docs)\n",
    "    \n",
    "    idf = {}\n",
    "    \n",
    "    for term in df:\n",
    "        n = len([doc for doc in tokenized_docs if term in doc])\n",
    "        idf[term] = math.log(N / float(df[term] + 1)) #math.log(1 + (N - n + 0.5)/(n + 0.5))\n",
    "    return idf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculateIDFBM25(tokenized_docs):\n",
    "    #dictionary with the frequency of each term in all documents\n",
    "    all_terms = [term for doc in tokenized_docs for term in doc]\n",
    "    \n",
    "    df = defaultdict(int)\n",
    "\n",
    "    # Calculate DF for each term\n",
    "    for term in all_terms:\n",
    "        df[term] += 1\n",
    "    # Calculate IDF for each term\n",
    "    N = len(tokenized_docs)\n",
    "    \n",
    "    idf = {}\n",
    "    \n",
    "    for term in df:\n",
    "        n = len([doc for doc in tokenized_docs if term in doc])\n",
    "        idf[term] = math.log((N - n + 0.5)/(n + 0.5)+1)\n",
    "    \n",
    "    return idf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#idf = calculateIDFBM25(tokenized_docs)\n",
    "idf = calculateIDF(tokenized_docs,term_freqs)\n",
    "print(idf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00052125",
   "metadata": {},
   "source": [
    "# Step successivi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fac39c",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73a901f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF- DOCUMENTS:\n",
      "  (0, 153)\t0.03366972268896855\n",
      "  (0, 74)\t0.0473216204390231\n",
      "  (0, 109)\t0.03366972268896855\n",
      "  (0, 50)\t0.0473216204390231\n",
      "  (0, 144)\t0.0473216204390231\n",
      "  (0, 169)\t0.0473216204390231\n",
      "  (0, 42)\t0.0473216204390231\n",
      "  (0, 21)\t0.0473216204390231\n",
      "  (0, 184)\t0.0473216204390231\n",
      "  (0, 43)\t0.0473216204390231\n",
      "  (0, 165)\t0.0473216204390231\n",
      "  (0, 66)\t0.0473216204390231\n",
      "  (0, 68)\t0.0473216204390231\n",
      "  (0, 100)\t0.03366972268896855\n",
      "  (0, 129)\t0.0946432408780462\n",
      "  (0, 69)\t0.0473216204390231\n",
      "  (0, 36)\t0.0473216204390231\n",
      "  (0, 150)\t0.0473216204390231\n",
      "  (0, 30)\t0.0473216204390231\n",
      "  (0, 60)\t0.0473216204390231\n",
      "  (0, 55)\t0.0473216204390231\n",
      "  (0, 54)\t0.0473216204390231\n",
      "  (0, 93)\t0.0473216204390231\n",
      "  (0, 29)\t0.0473216204390231\n",
      "  (0, 61)\t0.0473216204390231\n",
      "  :\t:\n",
      "  (1, 174)\t0.0883131559753662\n",
      "  (1, 31)\t0.0883131559753662\n",
      "  (1, 134)\t0.0883131559753662\n",
      "  (1, 143)\t0.0883131559753662\n",
      "  (1, 47)\t0.0883131559753662\n",
      "  (1, 181)\t0.1766263119507324\n",
      "  (1, 49)\t0.0883131559753662\n",
      "  (1, 26)\t0.1766263119507324\n",
      "  (1, 76)\t0.1766263119507324\n",
      "  (1, 163)\t0.0883131559753662\n",
      "  (1, 78)\t0.0883131559753662\n",
      "  (1, 1)\t0.0883131559753662\n",
      "  (1, 96)\t0.0883131559753662\n",
      "  (1, 0)\t0.0883131559753662\n",
      "  (1, 46)\t0.0883131559753662\n",
      "  (1, 45)\t0.0883131559753662\n",
      "  (1, 146)\t0.0883131559753662\n",
      "  (1, 153)\t0.0628355378343335\n",
      "  (1, 109)\t0.0628355378343335\n",
      "  (1, 100)\t0.125671075668667\n",
      "  (1, 167)\t0.37701322700600104\n",
      "  (1, 23)\t0.125671075668667\n",
      "  (1, 108)\t0.4398487648403345\n",
      "  (1, 11)\t0.251342151337334\n",
      "  (1, 70)\t0.125671075668667\n",
      "TFIDF- QUERY:\n",
      "  (0, 167)\t0.5015489070943787\n",
      "  (0, 108)\t0.5015489070943787\n",
      "  (0, 81)\t0.7049094889309326\n",
      "  (1, 171)\t0.46977773849858007\n",
      "  (1, 167)\t0.6685014601763077\n",
      "  (1, 141)\t0.46977773849858007\n",
      "  (1, 70)\t0.33425073008815387\n",
      "  (2, 184)\t0.5773502691896257\n",
      "  (2, 126)\t0.5773502691896257\n",
      "  (2, 81)\t0.5773502691896257\n",
      "  (3, 60)\t1.0\n",
      "  (4, 159)\t0.6316672017376245\n",
      "  (4, 70)\t0.4494364165239821\n",
      "  (4, 6)\t0.6316672017376245\n",
      "  (5, 108)\t0.5797386715376657\n",
      "  (5, 59)\t0.8148024746671689\n",
      "  (6, 171)\t0.7071067811865476\n",
      "  (6, 166)\t0.7071067811865476\n",
      "  (7, 171)\t0.424239626529491\n",
      "  (7, 108)\t0.9055499650942062\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def calculateTFIDF(tokenized_docs):\n",
    "    # Initialize the TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in tokenized_docs])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names,vectorizer\n",
    " \n",
    "    \n",
    "def calculateTFIDFQ(tokenized_query,vectorizer):\n",
    "    # Initialize the TfidfVectorizer\n",
    "    #vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.transform([' '.join(doc) for doc in tokenized_query])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names\n",
    "\n",
    "    \n",
    "\n",
    "tfidf_matrix_docs, feature_names_docs,vectorizer  = calculateTFIDF(tokenized_docs)\n",
    "tfidf_matrix_query, feature_names_query  = calculateTFIDFQ( tokenized_query,vectorizer)\n",
    "\n",
    "print(\"TFIDF- DOCUMENTS:\")\n",
    "#print(tfidf_matrix_docs.toarray())\n",
    "print(tfidf_matrix_docs)\n",
    "\n",
    "print(\"TFIDF- QUERY:\")\n",
    "#print(tfidf_matrix_query.toarray())\n",
    "print(tfidf_matrix_query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e1b30",
   "metadata": {},
   "source": [
    "# Dot Product of Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "92de26cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3995525270.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\lita4\\AppData\\Local\\Temp\\ipykernel_18036\\3995525270.py\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    for i in tfidf_matrix_query[0,i].toarray()\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_Dot_Product(tfidf_matrix_docs,tfidf_matrix_query):\n",
    "    \n",
    "    for i in len\n",
    "    results = np.dot(tfidf_matrix_docs,tfidf_matrix_query[0])\n",
    "    return results\n",
    "\n",
    "dot_result = calculate_Dot_Product(tfidf_matrix_docs,tfidf_matrix_query)\n",
    "print(dot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5aaaa98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0687327120513277, 3.0164868921587815]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def bm25_tf_idf(tokenized_docs, term_freqs, idf):\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    N = len(tokenized_docs)\n",
    "    avg_doc_len = sum(len(doc) for doc in tokenized_docs) / N\n",
    "    scores = []\n",
    "    for i in range(N):\n",
    "        score = 0\n",
    "        doc = tokenized_docs[i]\n",
    "        doc_len = len(doc)\n",
    "        for term, freq in term_freqs[i].items():\n",
    "            if term in idf:\n",
    "                idf_val = idf[term]\n",
    "                tf = ((k1 + 1) * freq) / (k1 * ((1 - b) + (b * (doc_len / avg_doc_len))) + freq)\n",
    "                score += tf * idf_val\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "score = bm25_tf_idf(tokenized_docs, term_freqs, idf)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78d03c",
   "metadata": {},
   "source": [
    "# Sparse Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27080096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "131a3cc7",
   "metadata": {},
   "source": [
    "# Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "567aef90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "microsoft/minilm-l6-hybrid-512 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/microsoft/minilm-l6-hybrid-512/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1165\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[0;32m   1167\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1506\u001b[0m     )\n\u001b[1;32m-> 1507\u001b[1;33m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    290\u001b[0m             )\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6435421e-1bb4c6ad51aa7bd92a8a6462)\n\nRepository Not Found for url: https://huggingface.co/microsoft/minilm-l6-hybrid-512/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18036\\2959885414.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load tokenizer and model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"microsoft/minilm-l6-hybrid-512\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"microsoft/minilm-l6-hybrid-512\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[1;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[0mtokenizer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"_commit_hash\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m     ```\"\"\"\n\u001b[0;32m    462\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m     resolved_config_file = cached_file(\n\u001b[0m\u001b[0;32m    464\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         raise EnvironmentError(\n\u001b[0m\u001b[0;32m    425\u001b[0m             \u001b[1;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[1;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: microsoft/minilm-l6-hybrid-512 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/minilm-l6-hybrid-512\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/minilm-l6-hybrid-512\")\n",
    "\n",
    "# Define function to convert an array of text into dense vectors\n",
    "def create_dense_vectors(inputs):\n",
    "    # Tokenize the texts\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Pass the inputs through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_docs, output_hidden_states=True)\n",
    "\n",
    "    # Get the last hidden state of the model as the dense vector\n",
    "    dense_vectors = outputs.hidden_states[-1][:, 0, :].numpy()\n",
    "\n",
    "    print(dense_vectors.tolist())\n",
    "    return dense_vectors.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc5ce1",
   "metadata": {},
   "source": [
    "# Prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbb07163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query:  vaccine for covid19\n",
      "Top-k documents based on original query:  [' ']\n",
      "Expanded query:  vaccine for covid19 we\n",
      "Top-k documents based on expanded query:  [' ']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Define the documents and queries\n",
    "docs = [\" \", \"covid can kill people\",\"covid19 is a bluf\", \"This is the third document\",\"exist a vaccine to save the people from covid\",\"we mus us vaccine cause covid kill people\"]\n",
    "query = \"vaccine for covid19\"\n",
    "\n",
    "# Initialize the vectorizer and compute the TF-IDF matrix for the documents\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_docs = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Compute the TF-IDF matrix for the query\n",
    "tfidf_matrix_query = vectorizer.transform([query])\n",
    "\n",
    "# Compute the dot product between the documents and the query\n",
    "dot_product = np.dot(tfidf_matrix_docs, tfidf_matrix_query.T)\n",
    "\n",
    "# Get the top-k documents based on the dot product\n",
    "k = 1\n",
    "top_k_indices = np.argsort(dot_product, axis=0)[-k:]\n",
    "top_k_docs = [docs[i] for i in top_k_indices]\n",
    "\n",
    "# Compute the average TF-IDF weights for the top-k terms in the top-k documents\n",
    "top_k_terms = set()\n",
    "for doc in top_k_docs:\n",
    "    doc_tfidf = vectorizer.transform([doc])\n",
    "    doc_weights = np.array(doc_tfidf.mean(axis=0))[0]\n",
    "    top_k_terms.update(np.argsort(doc_weights)[-k:])\n",
    "\n",
    "# Expand the query with the top-k terms\n",
    "expanded_query = query + \" \" + \" \".join([vectorizer.get_feature_names()[i] for i in top_k_terms])\n",
    "\n",
    "# Compute the TF-IDF matrix for the expanded query\n",
    "tfidf_matrix_expanded_query = vectorizer.transform([expanded_query])\n",
    "\n",
    "# Compute the dot product between the documents and the expanded query\n",
    "dot_product_expanded = np.dot(tfidf_matrix_docs, tfidf_matrix_expanded_query.T)\n",
    "\n",
    "# Get the top-k documents based on the expanded query\n",
    "top_k_indices_expanded = np.argsort(dot_product_expanded, axis=0)[-k:]\n",
    "top_k_docs_expanded = [docs[i] for i in top_k_indices_expanded]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original query: \", query)\n",
    "print(\"Top-k documents based on original query: \", top_k_docs)\n",
    "print(\"Expanded query: \", expanded_query)\n",
    "print(\"Top-k documents based on expanded query: \", top_k_docs_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be118f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
