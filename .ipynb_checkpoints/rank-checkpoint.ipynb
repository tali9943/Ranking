{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7d86ce",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbb2d2",
   "metadata": {},
   "source": [
    "Second Assignement of Learning with Massive Data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26dd9783",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1476\\1340473643.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load Sentence Transformer model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Sentence Transformer model\n",
    "model = SentenceTransformer('all-minilm-l6-v2')\n",
    "\n",
    "# Load dataset and create CountVectorizer object for sparse vectors\n",
    "corpus = [...]  # List of documents\n",
    "vectorizer = CountVectorizer()\n",
    "sparse_vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute BM25 weights for each document (to be used as sparse vectors)\n",
    "doc_freqs = np.array(sparse_vectors.sum(axis=0))[0]\n",
    "doc_lengths = np.array(sparse_vectors.sum(axis=1)).reshape(-1)\n",
    "avg_doc_length = np.mean(doc_lengths)\n",
    "k1 = 1.2\n",
    "b = 0.75\n",
    "idf = np.log((len(corpus) - doc_freqs + 0.5) / (doc_freqs + 0.5))\n",
    "bm25_weights = ((sparse_vectors * (k1 + 1)) / (sparse_vectors + k1 * (1 - b + b * doc_lengths / avg_doc_length))) * idf\n",
    "\n",
    "# Create dense vectors for each document\n",
    "dense_vectors = model.encode(corpus)\n",
    "\n",
    "def retrieve_top_k(query, k):\n",
    "    # Compute inner product of query with all dense vectors\n",
    "    dense_scores = np.dot(query[0], dense_vectors.T)\n",
    "\n",
    "    # Compute inner product of query with all sparse vectors (BM25 weights)\n",
    "    sparse_query = vectorizer.transform([query[1]])\n",
    "    sparse_scores = np.dot(bm25_weights, sparse_query.T).flatten()\n",
    "\n",
    "    # Merge the two scores and retrieve the top-k documents\n",
    "    all_scores = dense_scores + sparse_scores\n",
    "    top_k_indices = np.argsort(-all_scores)[:k]\n",
    "    top_k_scores = all_scores[top_k_indices]\n",
    "    top_k_documents = [corpus[i] for i in top_k_indices]\n",
    "\n",
    "    return top_k_documents, top_k_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade98943",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2820a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'student', 'student', 'stude', 'studi', 'heme', 'oxygenas', 'ho', 'induc', 'stress', 'protein', 'confer', 'cytoprotect', 'oxid', 'stress', 'vitro', 'vivo', 'addit', 'physiolog', 'role', 'heme', 'degrad', 'ho', 'may', 'influenc', 'number', 'cellular', 'process', 'includ', 'growth', 'inflamm', 'apoptosi', 'virtu', 'antiinflammatori', 'effect', 'ho', 'limit', 'tissu', 'damag', 'respons', 'proinflammatori', 'stimulu', 'prevent', 'allograft', 'reject', 'transplant', 'transcript', 'upregul', 'ho', 'respond', 'mani', 'agent', 'hypoxia', 'bacteri', 'lipopolysaccharid', 'reactiv', 'oxygennitrogen', 'speci', 'ho', 'constitut', 'express', 'isozym', 'heme', 'oxygenas', 'catalyz', 'ratelimit', 'step', 'convers', 'heme', 'metabolit', 'bilirubin', 'ixÎ±', 'ferrou', 'iron', 'carbon', 'monoxid', 'co', 'mechan', 'ho', 'provid', 'protect', 'like', 'involv', 'enzymat', 'reaction', 'product', 'remark', 'administr', 'co', 'low', 'concentr', 'substitut', 'ho', 'respect', 'antiinflammatori', 'antiapoptot', 'effect', 'suggest', 'role', 'co', 'key', 'mediat', 'ho', 'function', 'chronic', 'lowlevel', 'exogen', 'exposur', 'co', 'cigarett', 'smoke', 'contribut', 'import', 'co', 'pulmonari', 'medicin', 'implic', 'hoco', 'system', 'pulmonari', 'diseas', 'discuss', 'review', 'emphasi', 'inflammatori', 'state']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stemmingLemming(filtered_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Perform stemming or lemmatization on filtered tokens\n",
    "    \n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    # Print the stemmed or lemmatized tokens\n",
    "    #print(filtered_tokens)\n",
    "    #print(lemmatized_tokens)\n",
    "    return filtered_tokens\n",
    "    \n",
    "path_name = './Databases/trec-covid/gigi.jsonl'\n",
    "   \n",
    "def tokenize(path_name):\n",
    "    # Open the JSONL file\n",
    "    with open(path_name, 'r') as file:      \n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            text = data['text']\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            text = re.sub(r'\\d+', '', text)  # Remove all numbers\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove all punctuation\n",
    "            tokens = word_tokenize(text)  # Tokenize the text\n",
    "            filtered_tokens = [token for token in tokens if token not in stop_words]  # Remove all stop words\n",
    "            \n",
    "            tokens = stemmingLemming(filtered_tokens)\n",
    "            \n",
    "            print(tokens)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "tokenize(path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771cb0db",
   "metadata": {},
   "source": [
    "# Step successivi\n",
    "\n",
    "-creare un vocabolario \\\\\n",
    "-calcolare le frequenze tf e idf tf (creare vettore)  sparso e denso \\\\\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd88b5",
   "metadata": {},
   "source": [
    "# Sparse Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74efb178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a671432",
   "metadata": {},
   "source": [
    "# Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb3824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
