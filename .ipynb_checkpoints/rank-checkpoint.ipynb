{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7d86ce",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbb2d2",
   "metadata": {},
   "source": [
    "Second Assignement of Learning with Massive Data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fe59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters(K,K_p,n_query):\n",
    "    K = K\n",
    "    K_p = K_p\n",
    "    n_query = n_query\n",
    "    \n",
    "    return K, K_p, n_query\n",
    "\n",
    "\n",
    "\n",
    "K, K_p, n_query = choose_parameters(K,K_p,n_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cb979",
   "metadata": {},
   "source": [
    "# Usefull Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b86d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def score_table(score):\n",
    "    df = pd.DataFrame(score)\n",
    "\n",
    "    df.index = [\"Query \" + str(i+1) for i in range(len(df.index))]\n",
    "    df.columns = [\"Document \" + str(i+1) for i in range(len(df.columns))]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def printDocsQuery(documents,queries,results_k,list_top_k):  \n",
    "    for query in queries:   \n",
    "        print(\"QUERY: \"+ query[1])\n",
    "        print()\n",
    "        elem = int(query[0])-1\n",
    "        doc = [a for a, b in list_top_k[elem]]\n",
    "        for position in doc:\n",
    "            #print(documents[position][0])\n",
    "            print(position)\n",
    "            print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101da09",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26dd9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "path_name_documents = './Databases/prova/gigi.jsonl'\n",
    "path_name_query = './Databases/prova/query.jsonl'\n",
    "\n",
    "\n",
    "def readFile(path_name):\n",
    "    # Load the JSONL file into a list\n",
    "    with open(path_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Convert each JSON object into a dictionary\n",
    "    dicts = [json.loads(line) for line in lines]\n",
    "\n",
    "    # Convert the dictionaries into arrays and stack them vertically\n",
    "    arrays = np.vstack([np.array(list(d.values())) for d in dicts])\n",
    "\n",
    "    # Convert the arrays into a list of lists\n",
    "    text = arrays.tolist()\n",
    "    \n",
    "    return text\n",
    "\n",
    "documents = readFile(path_name_documents)\n",
    "queries = readFile(path_name_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade98943",
   "metadata": {},
   "source": [
    "# Tokenize, Stemming and Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2820a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stemmingLemming(filtered_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Perform stemming or lemmatization on filtered tokens\n",
    "    \n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return filtered_tokens\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "def tokenize(path_name):\n",
    "    \n",
    "    with open(path_name, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "        # Create an empty list to store the tokenized documents\n",
    "        tokenized_docs = []\n",
    "\n",
    "        # Loop through each line in the JSONL file\n",
    "        for line in data:\n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            doc = json.loads(line)\n",
    "\n",
    "            # Extract the text from the dictionary\n",
    "            text = doc['text']\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            #text = re.sub(r'\\d+', '', text)  # Remove all numbers\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove all punctuation\n",
    "\n",
    "            # Tokenize the text using NLTK\n",
    "            tokens = word_tokenize(text)\n",
    "            tokensStemLem = stemmingLemming(tokens)\n",
    "\n",
    "            # Add the tokenized document to the list\n",
    "            tokenized_docs.append(tokensStemLem)\n",
    "\n",
    "        # Print the tokenized documents\n",
    "    return tokenized_docs\n",
    "\n",
    "\n",
    "tokenized_docs = tokenize(path_name_documents)\n",
    "tokenized_query = tokenize(path_name_query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00052125",
   "metadata": {},
   "source": [
    "# Sparse Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fac39c",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a901f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lita4\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def calculateTFIDF(tokenized_docs):\n",
    "    # Initialize the TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in tokenized_docs])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names,vectorizer\n",
    " \n",
    "    \n",
    "    \n",
    "def calculateTFIDFQuery(tokenized_query,vectorizer):\n",
    "    \n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.transform([' '.join(doc) for doc in tokenized_query])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names\n",
    "\n",
    "    \n",
    "\n",
    "tfidf_matrix_docs, feature_names_docs,vectorizer  = calculateTFIDF(tokenized_docs)\n",
    "tfidf_matrix_query, feature_names_query  = calculateTFIDFQuery( tokenized_query,vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e1b30",
   "metadata": {},
   "source": [
    "### Dot Product for Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92de26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_Dot_Product_Sparse(tfidf_matrix_docs, tfidf_matrix_query):\n",
    "    matrix_docs = tfidf_matrix_docs.toarray()\n",
    "    matrix_query = tfidf_matrix_query.toarray()\n",
    "    num_queries = matrix_query.shape[0]\n",
    "    num_docs = matrix_docs.shape[0]\n",
    "    sparse_score_results = []\n",
    "    \n",
    "    for query in range(num_queries):\n",
    "        sparse_score_docs = []\n",
    "        for doc in range(num_docs):\n",
    "            dot_result = np.dot(matrix_query[query], matrix_docs[doc])\n",
    "            sparse_score_docs.append(dot_result)\n",
    "        sparse_score_results.append(sparse_score_docs)\n",
    "        \n",
    "    return sparse_score_results\n",
    "\n",
    "\n",
    "\n",
    "sparse_score_results = calculate_Dot_Product_Sparse(tfidf_matrix_docs,tfidf_matrix_query)\n",
    "sparse_score_table = score_table(sparse_score_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a3cc7",
   "metadata": {},
   "source": [
    "# Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567aef90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def create_dense_vector(document):\n",
    "    # Load the all-MiniLM-L6-v2 model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Create a dense vector representation of the document\n",
    "    dense_vector = model.encode(document, convert_to_tensor=True)\n",
    "    \n",
    "    \n",
    "    return dense_vector\n",
    "\n",
    "\n",
    "dense_vector_document = create_dense_vector(documents)\n",
    "dense_vector_query = create_dense_vector(queries)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc5ce1",
   "metadata": {},
   "source": [
    "### Dot Product for Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbb07163",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_Dot_Product_Dense(dense_vector_document, dense_vector_query):\n",
    "    num_queries = dense_vector_query.shape[0]\n",
    "    num_docs = dense_vector_document.shape[0]\n",
    "    dense_score_results = []\n",
    "    \n",
    "    for query in range(num_queries):\n",
    "        dense_score_docs = []\n",
    "        for doc in range(num_docs):\n",
    "            dot_result = np.dot(dense_vector_query[query], dense_vector_document[doc])\n",
    "            dense_score_docs.append(dot_result)\n",
    "        dense_score_results.append(dense_score_docs)\n",
    "        \n",
    "    return dense_score_results\n",
    "\n",
    "\n",
    "dense_score_results = calculate_Dot_Product_Dense(dense_vector_document, dense_vector_query)\n",
    "\n",
    "dense_score_table = score_table(dense_score_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ed758",
   "metadata": {},
   "source": [
    "# Calculate the final score for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9455aba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document 1</th>\n",
       "      <th>Document 2</th>\n",
       "      <th>Document 3</th>\n",
       "      <th>Document 4</th>\n",
       "      <th>Document 5</th>\n",
       "      <th>Document 6</th>\n",
       "      <th>Document 7</th>\n",
       "      <th>Document 8</th>\n",
       "      <th>Document 9</th>\n",
       "      <th>Document 10</th>\n",
       "      <th>Document 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Query 1</th>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.377585</td>\n",
       "      <td>0.269571</td>\n",
       "      <td>0.473410</td>\n",
       "      <td>0.603978</td>\n",
       "      <td>0.341799</td>\n",
       "      <td>0.493245</td>\n",
       "      <td>0.435917</td>\n",
       "      <td>0.136205</td>\n",
       "      <td>0.373188</td>\n",
       "      <td>0.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Query 2</th>\n",
       "      <td>0.324503</td>\n",
       "      <td>0.219681</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>0.296075</td>\n",
       "      <td>0.505359</td>\n",
       "      <td>0.200209</td>\n",
       "      <td>0.481966</td>\n",
       "      <td>0.348569</td>\n",
       "      <td>0.281434</td>\n",
       "      <td>0.228287</td>\n",
       "      <td>0.324503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Query 3</th>\n",
       "      <td>0.153645</td>\n",
       "      <td>0.119676</td>\n",
       "      <td>0.285546</td>\n",
       "      <td>0.139068</td>\n",
       "      <td>0.416846</td>\n",
       "      <td>0.187099</td>\n",
       "      <td>0.356040</td>\n",
       "      <td>0.207124</td>\n",
       "      <td>0.195489</td>\n",
       "      <td>0.120588</td>\n",
       "      <td>0.153645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Query 4</th>\n",
       "      <td>0.166681</td>\n",
       "      <td>0.175452</td>\n",
       "      <td>0.157920</td>\n",
       "      <td>0.226517</td>\n",
       "      <td>0.266544</td>\n",
       "      <td>0.214995</td>\n",
       "      <td>0.428582</td>\n",
       "      <td>0.259010</td>\n",
       "      <td>0.168591</td>\n",
       "      <td>0.175452</td>\n",
       "      <td>0.166681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Query 5</th>\n",
       "      <td>0.290604</td>\n",
       "      <td>0.205079</td>\n",
       "      <td>0.105684</td>\n",
       "      <td>0.275287</td>\n",
       "      <td>0.378720</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>0.261496</td>\n",
       "      <td>0.183062</td>\n",
       "      <td>0.194384</td>\n",
       "      <td>0.205948</td>\n",
       "      <td>0.290604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Query 6</th>\n",
       "      <td>0.166459</td>\n",
       "      <td>0.209536</td>\n",
       "      <td>0.207605</td>\n",
       "      <td>0.192798</td>\n",
       "      <td>0.254442</td>\n",
       "      <td>0.159554</td>\n",
       "      <td>0.256521</td>\n",
       "      <td>0.227439</td>\n",
       "      <td>0.267136</td>\n",
       "      <td>0.203719</td>\n",
       "      <td>0.166459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Query 7</th>\n",
       "      <td>0.250586</td>\n",
       "      <td>0.194642</td>\n",
       "      <td>0.244185</td>\n",
       "      <td>0.220003</td>\n",
       "      <td>0.361672</td>\n",
       "      <td>0.188566</td>\n",
       "      <td>0.444623</td>\n",
       "      <td>0.243737</td>\n",
       "      <td>0.371042</td>\n",
       "      <td>0.195603</td>\n",
       "      <td>0.250586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Query 8</th>\n",
       "      <td>0.336789</td>\n",
       "      <td>0.355880</td>\n",
       "      <td>0.253950</td>\n",
       "      <td>0.406531</td>\n",
       "      <td>0.426588</td>\n",
       "      <td>0.289356</td>\n",
       "      <td>0.355287</td>\n",
       "      <td>0.303702</td>\n",
       "      <td>0.409178</td>\n",
       "      <td>0.335245</td>\n",
       "      <td>0.336789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Document 1  Document 2  Document 3  Document 4  Document 5  \\\n",
       "Query 1    0.266100    0.377585    0.269571    0.473410    0.603978   \n",
       "Query 2    0.324503    0.219681    0.169559    0.296075    0.505359   \n",
       "Query 3    0.153645    0.119676    0.285546    0.139068    0.416846   \n",
       "Query 4    0.166681    0.175452    0.157920    0.226517    0.266544   \n",
       "Query 5    0.290604    0.205079    0.105684    0.275287    0.378720   \n",
       "Query 6    0.166459    0.209536    0.207605    0.192798    0.254442   \n",
       "Query 7    0.250586    0.194642    0.244185    0.220003    0.361672   \n",
       "Query 8    0.336789    0.355880    0.253950    0.406531    0.426588   \n",
       "\n",
       "         Document 6  Document 7  Document 8  Document 9  Document 10  \\\n",
       "Query 1    0.341799    0.493245    0.435917    0.136205     0.373188   \n",
       "Query 2    0.200209    0.481966    0.348569    0.281434     0.228287   \n",
       "Query 3    0.187099    0.356040    0.207124    0.195489     0.120588   \n",
       "Query 4    0.214995    0.428582    0.259010    0.168591     0.175452   \n",
       "Query 5    0.325200    0.261496    0.183062    0.194384     0.205948   \n",
       "Query 6    0.159554    0.256521    0.227439    0.267136     0.203719   \n",
       "Query 7    0.188566    0.444623    0.243737    0.371042     0.195603   \n",
       "Query 8    0.289356    0.355287    0.303702    0.409178     0.335245   \n",
       "\n",
       "         Document 11  \n",
       "Query 1     0.266100  \n",
       "Query 2     0.324503  \n",
       "Query 3     0.153645  \n",
       "Query 4     0.166681  \n",
       "Query 5     0.290604  \n",
       "Query 6     0.166459  \n",
       "Query 7     0.250586  \n",
       "Query 8     0.336789  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_full_score(sparse_score_results,dense_score_results):\n",
    "    sparse_vector = np.stack(sparse_score_results)\n",
    "    dense_vector = np.stack(dense_score_results)\n",
    "    full_vector = sparse_vector + dense_vector\n",
    "    \n",
    "    return full_vector, dense_vector,sparse_vector\n",
    "    \n",
    "full_vector, dense_vector,sparse_vector = calculate_full_score(sparse_score_results,dense_score_results)\n",
    "\n",
    "df = table_final_score(full_vector)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7961c0f",
   "metadata": {},
   "source": [
    "# Select query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf0c4f",
   "metadata": {},
   "source": [
    "Choose the query that you want analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9fc3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def choose_query(n_query):\n",
    "    score_sparse = sparse_vector [n_query]\n",
    "    score_dense = dense_vector [n_query]\n",
    "    score_full = full_vector [n_query]\n",
    "    \n",
    "    return score_sparse, score_dense,score_full\n",
    "\n",
    "score_sparse, score_dense,score_full = choose_query(n_query)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d337c8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 3, 7], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take the position of top k score\n",
    "def ground_truth_tok_k(K):\n",
    "    ground_truth = np.argsort(score_full)[::-1][:K]\n",
    "    \n",
    "    return ground_truth \n",
    "\n",
    "ground_truth = ground_truth_tok_k(K)\n",
    "ground_truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f5d55",
   "metadata": {},
   "source": [
    "# For sparse and dense vector we take the best k_p elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e088ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_top_k_first(K_p,score_dense,score_sparse):\n",
    "    \n",
    "    best_dense = np.argsort(score_dense)[::-1][:K_p]\n",
    "    best_sparse = np.argsort(score_sparse)[::-1][:K_p]\n",
    "    \n",
    "    return best_sparse, best_dense\n",
    "\n",
    "\n",
    "best_sparse, best_dense = best_top_k_first(K_p,score_dense,score_sparse)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a71a7",
   "metadata": {},
   "source": [
    "# Union, without duplicate, of best sparse and best dense elements of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9edd2dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6, 7, 9]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def union_best_vectors(best_sparse, best_dense):\n",
    "    best_index_score = list(set(list(best_sparse) + list(best_dense))) #union\n",
    "    return best_index_score\n",
    "\n",
    "best_index_score = union_best_vectors(best_sparse, best_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2786c",
   "metadata": {},
   "source": [
    "# Take the best score from the final score list using the indexes of sparse and dense union list and also return the best indexes sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aa728da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 2, 5, 0, 6, 1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_full_score(best_index_score):\n",
    "    \n",
    "    best_full_score = score_full[best_index_score]\n",
    "    order_best_full_score = np.argsort(best_full_score)[::-1]  #questo lo ordino per ordinare i miei best\n",
    "        \n",
    "    return best_full_score, order_best_full_score\n",
    "\n",
    "best_full_score, order_best_full_score = best_full_score(best_index_score)\n",
    "\n",
    "order_best_full_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd5946",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# create a list with the indexes of best score ordered and take the first K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04629ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def retrieved_K_indexes(order_best_full_score):\n",
    "    actual_order = list(order_best_full_score)\n",
    "    k_indexes_ordered = actual_order[:K]\n",
    "    \n",
    "    return k_indexes_ordered\n",
    "\n",
    "k_indexes_ordered = retrieved_K_indexes(order_best_full_score)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16864f6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_relevant_docs(k_indexes_ordered,ground_truth):\n",
    "    relevant = set(retrieved).intersection(ground_truth)\n",
    "    real_relevant_docs =len(relevant)/len(ground_truth)\n",
    "\n",
    "    return real_relevant_docs\n",
    "\n",
    "real_relevant_docs = calculate_relevant_docs(k_indexes_ordered,ground_truth)\n",
    "    \n",
    "real_relevant_docs\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
