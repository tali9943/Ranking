{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7d86ce",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbb2d2",
   "metadata": {},
   "source": [
    "Second Assignement of Learning with Massive Data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade98943",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2820a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['play', 'play', 'play', 'student', 'student', 'stude', 'studi', 'heme', 'oxygenas', 'ho', 'an', 'induc', 'stress', 'protein', 'confer', 'cytoprotect', 'against', 'oxid', 'stress', 'in', 'vitro', 'and', 'in', 'vivo', 'in', 'addit', 'to', 'it', 'physiolog', 'role', 'in', 'heme', 'degrad', 'ho', 'may', 'influenc', 'a', 'number', 'of', 'cellular', 'process', 'includ', 'growth', 'inflamm', 'and', 'apoptosi', 'by', 'virtu', 'of', 'antiinflammatori', 'effect', 'ho', 'limit', 'tissu', 'damag', 'in', 'respons', 'to', 'proinflammatori', 'stimulu', 'and', 'prevent', 'allograft', 'reject', 'after', 'transplant', 'the', 'transcript', 'upregul', 'of', 'ho', 'respond', 'to', 'mani', 'agent', 'such', 'a', 'hypoxia', 'bacteri', 'lipopolysaccharid', 'and', 'reactiv', 'oxygennitrogen', 'speci', 'ho', 'and', 'it', 'constitut', 'express', 'isozym', 'heme', 'oxygenas', 'catalyz', 'the', 'ratelimit', 'step', 'in', 'the', 'convers', 'of', 'heme', 'to', 'it', 'metabolit', 'bilirubin', 'ixα', 'ferrou', 'iron', 'and', 'carbon', 'monoxid', 'co', 'the', 'mechan', 'by', 'which', 'ho', 'provid', 'protect', 'most', 'like', 'involv', 'it', 'enzymat', 'reaction', 'product', 'remark', 'administr', 'of', 'co', 'at', 'low', 'concentr', 'can', 'substitut', 'for', 'ho', 'with', 'respect', 'to', 'antiinflammatori', 'and', 'antiapoptot', 'effect', 'suggest', 'a', 'role', 'for', 'co', 'a', 'a', 'key', 'mediat', 'of', 'ho', 'function', 'chronic', 'lowlevel', 'exogen', 'exposur', 'to', 'co', 'from', 'cigarett', 'smoke', 'contribut', 'to', 'the', 'import', 'of', 'co', 'in', 'pulmonari', 'medicin', 'the', 'implic', 'of', 'the', 'hoco', 'system', 'in', 'pulmonari', 'diseas', 'will', 'be', 'discuss', 'in', 'thi', 'review', 'with', 'an', 'emphasi', 'on', 'inflammatori', 'state'], ['second', 'document', 'documnent', 'of', 'mart', 'the', 'st', 'intern', 'symposium', 'on', 'intens', 'care', 'and', 'emerg', 'medicin', 'wa', 'domin', 'by', 'the', 'result', 'of', 'recent', 'clinic', 'trial', 'in', 'sepsi', 'and', 'acut', 'respiratori', 'distress', 'syndrom', 'ard', 'the', 'promis', 'of', 'extracorpor', 'liver', 'replac', 'therapi', 'and', 'noninvas', 'ventil', 'were', 'other', 'area', 'of', 'interest', 'ethic', 'issu', 'also', 'receiv', 'attent', 'overal', 'the', 'state', 'of', 'the', 'art', 'lectur', 'procon', 'debat', 'seminar', 'and', 'tutori', 'were', 'of', 'a', 'high', 'standard', 'the', 'meet', 'wa', 'mark', 'by', 'a', 'sens', 'of', 'renew', 'enthusiasm', 'that', 'posit', 'progress', 'is', 'occur', 'in', 'intens', 'care', 'medicin']]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stemmingLemming(filtered_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Perform stemming or lemmatization on filtered tokens\n",
    "    \n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return filtered_tokens\n",
    "    \n",
    "path_name = './Databases/trec-covid/gigi.jsonl'\n",
    " \n",
    "    \n",
    "\n",
    "def tokenize(path_name):\n",
    "    \n",
    "    with open(path_name, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "        # Create an empty list to store the tokenized documents\n",
    "        tokenized_docs = []\n",
    "\n",
    "        # Loop through each line in the JSONL file\n",
    "        for line in data:\n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            doc = json.loads(line)\n",
    "\n",
    "            # Extract the text from the dictionary\n",
    "            text = doc['text']\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            text = re.sub(r'\\d+', '', text)  # Remove all numbers\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove all punctuation\n",
    "\n",
    "            # Tokenize the text using NLTK\n",
    "            tokens = word_tokenize(text)\n",
    "            tokensStemLem = stemmingLemming(tokens)\n",
    "\n",
    "            # Add the tokenized document to the list\n",
    "            tokenized_docs.append(tokensStemLem)\n",
    "\n",
    "        # Print the tokenized documents\n",
    "    return tokenized_docs\n",
    "\n",
    "tokenized_docs = tokenize(path_name)\n",
    "print(tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72f5c9",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8650446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'in': 0.046153846153846156, 'ho': 0.041025641025641026, 'of': 0.041025641025641026, 'and': 0.035897435897435895, 'to': 0.035897435897435895, 'the': 0.035897435897435895, 'a': 0.02564102564102564, 'co': 0.02564102564102564, 'heme': 0.020512820512820513, 'it': 0.020512820512820513, 'play': 0.015384615384615385, 'student': 0.010256410256410256, 'oxygenas': 0.010256410256410256, 'an': 0.010256410256410256, 'stress': 0.010256410256410256, 'role': 0.010256410256410256, 'by': 0.010256410256410256, 'antiinflammatori': 0.010256410256410256, 'effect': 0.010256410256410256, 'for': 0.010256410256410256, 'with': 0.010256410256410256, 'pulmonari': 0.010256410256410256, 'stude': 0.005128205128205128, 'studi': 0.005128205128205128, 'induc': 0.005128205128205128, 'protein': 0.005128205128205128, 'confer': 0.005128205128205128, 'cytoprotect': 0.005128205128205128, 'against': 0.005128205128205128, 'oxid': 0.005128205128205128, 'vitro': 0.005128205128205128, 'vivo': 0.005128205128205128, 'addit': 0.005128205128205128, 'physiolog': 0.005128205128205128, 'degrad': 0.005128205128205128, 'may': 0.005128205128205128, 'influenc': 0.005128205128205128, 'number': 0.005128205128205128, 'cellular': 0.005128205128205128, 'process': 0.005128205128205128, 'includ': 0.005128205128205128, 'growth': 0.005128205128205128, 'inflamm': 0.005128205128205128, 'apoptosi': 0.005128205128205128, 'virtu': 0.005128205128205128, 'limit': 0.005128205128205128, 'tissu': 0.005128205128205128, 'damag': 0.005128205128205128, 'respons': 0.005128205128205128, 'proinflammatori': 0.005128205128205128, 'stimulu': 0.005128205128205128, 'prevent': 0.005128205128205128, 'allograft': 0.005128205128205128, 'reject': 0.005128205128205128, 'after': 0.005128205128205128, 'transplant': 0.005128205128205128, 'transcript': 0.005128205128205128, 'upregul': 0.005128205128205128, 'respond': 0.005128205128205128, 'mani': 0.005128205128205128, 'agent': 0.005128205128205128, 'such': 0.005128205128205128, 'hypoxia': 0.005128205128205128, 'bacteri': 0.005128205128205128, 'lipopolysaccharid': 0.005128205128205128, 'reactiv': 0.005128205128205128, 'oxygennitrogen': 0.005128205128205128, 'speci': 0.005128205128205128, 'constitut': 0.005128205128205128, 'express': 0.005128205128205128, 'isozym': 0.005128205128205128, 'catalyz': 0.005128205128205128, 'ratelimit': 0.005128205128205128, 'step': 0.005128205128205128, 'convers': 0.005128205128205128, 'metabolit': 0.005128205128205128, 'bilirubin': 0.005128205128205128, 'ixα': 0.005128205128205128, 'ferrou': 0.005128205128205128, 'iron': 0.005128205128205128, 'carbon': 0.005128205128205128, 'monoxid': 0.005128205128205128, 'mechan': 0.005128205128205128, 'which': 0.005128205128205128, 'provid': 0.005128205128205128, 'protect': 0.005128205128205128, 'most': 0.005128205128205128, 'like': 0.005128205128205128, 'involv': 0.005128205128205128, 'enzymat': 0.005128205128205128, 'reaction': 0.005128205128205128, 'product': 0.005128205128205128, 'remark': 0.005128205128205128, 'administr': 0.005128205128205128, 'at': 0.005128205128205128, 'low': 0.005128205128205128, 'concentr': 0.005128205128205128, 'can': 0.005128205128205128, 'substitut': 0.005128205128205128, 'respect': 0.005128205128205128, 'antiapoptot': 0.005128205128205128, 'suggest': 0.005128205128205128, 'key': 0.005128205128205128, 'mediat': 0.005128205128205128, 'function': 0.005128205128205128, 'chronic': 0.005128205128205128, 'lowlevel': 0.005128205128205128, 'exogen': 0.005128205128205128, 'exposur': 0.005128205128205128, 'from': 0.005128205128205128, 'cigarett': 0.005128205128205128, 'smoke': 0.005128205128205128, 'contribut': 0.005128205128205128, 'import': 0.005128205128205128, 'medicin': 0.005128205128205128, 'implic': 0.005128205128205128, 'hoco': 0.005128205128205128, 'system': 0.005128205128205128, 'diseas': 0.005128205128205128, 'will': 0.005128205128205128, 'be': 0.005128205128205128, 'discuss': 0.005128205128205128, 'thi': 0.005128205128205128, 'review': 0.005128205128205128, 'emphasi': 0.005128205128205128, 'on': 0.005128205128205128, 'inflammatori': 0.005128205128205128, 'state': 0.005128205128205128})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk\n",
    "\n",
    "def calculateTF(tokenized_docs):\n",
    "    term_freqs = []\n",
    "    for doc in tokenized_docs:\n",
    "        doc_freq = collections.Counter(doc)\n",
    "        \n",
    "        total_terms = len(doc)\n",
    "        for term in doc_freq:\n",
    "            doc_freq[term] /= float(total_terms)\n",
    "        term_freqs.append(doc_freq)\n",
    "\n",
    "    # Print the term frequency for the first document\n",
    "    #print(term_freqs[0])\n",
    "    return term_freqs[0]\n",
    "    \n",
    "term_freqs = calculateTF(tokenized_docs)\n",
    "print(term_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76afbcbc",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504063d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6829430103857037\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "\n",
    "\n",
    "def calculateIDF(tokenized_docs,term_freqs ):\n",
    "\n",
    "    # Create a dictionary with the frequency of each term in all documents\n",
    "    all_terms = [term for doc in tokenized_docs for term in doc]\n",
    "    df = collections.Counter(term_freqs)\n",
    "\n",
    "    # Calculate IDF for each term\n",
    "    N = len(tokenized_docs)\n",
    "    idf = {}\n",
    "    for term in df:\n",
    "        idf[term] = math.log(N / float(df[term] + 1))\n",
    "    # Print the IDF for the term \"document\"\n",
    "    print(idf[\"student\"])\n",
    "\n",
    "calculateIDF(tokenized_docs,term_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00052125",
   "metadata": {},
   "source": [
    "# Step successivi\n",
    "\n",
    "-creare un vocabolario \n",
    "\n",
    "-calcolare le frequenze tf e idf tf (creare vettore)  sparso(BM25) e denso( all-minilm-l6-v2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fac39c",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa33c636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.04721601 0.04721601 0.04721601 0.04721601 0.04721601\n",
      "  0.04721601 0.         0.09443201 0.23516204 0.04721601 0.09443201\n",
      "  0.04721601 0.         0.         0.         0.04721601 0.\n",
      "  0.04721601 0.04721601 0.04721601 0.06718915 0.04721601 0.04721601\n",
      "  0.         0.04721601 0.04721601 0.04721601 0.04721601 0.\n",
      "  0.23608003 0.04721601 0.04721601 0.04721601 0.04721601 0.04721601\n",
      "  0.04721601 0.04721601 0.         0.04721601 0.04721601 0.04721601\n",
      "  0.         0.         0.         0.         0.09443201 0.\n",
      "  0.04721601 0.         0.04721601 0.         0.04721601 0.04721601\n",
      "  0.04721601 0.         0.04721601 0.09443201 0.04721601 0.04721601\n",
      "  0.04721601 0.18886402 0.         0.37772805 0.04721601 0.04721601\n",
      "  0.04721601 0.04721601 0.30235119 0.04721601 0.04721601 0.04721601\n",
      "  0.04721601 0.04721601 0.         0.         0.         0.04721601\n",
      "  0.04721601 0.         0.04721601 0.         0.18886402 0.04721601\n",
      "  0.04721601 0.         0.04721601 0.04721601 0.04721601 0.\n",
      "  0.04721601 0.04721601 0.04721601 0.         0.         0.04721601\n",
      "  0.04721601 0.04721601 0.03359458 0.         0.04721601 0.04721601\n",
      "  0.04721601 0.         0.04721601 0.         0.26875662 0.03359458\n",
      "  0.         0.         0.04721601 0.09443201 0.04721601 0.04721601\n",
      "  0.14164802 0.         0.04721601 0.04721601 0.         0.04721601\n",
      "  0.         0.04721601 0.         0.04721601 0.04721601 0.04721601\n",
      "  0.09443201 0.04721601 0.04721601 0.04721601 0.         0.\n",
      "  0.04721601 0.04721601 0.         0.         0.04721601 0.\n",
      "  0.04721601 0.04721601 0.         0.04721601 0.09443201 0.\n",
      "  0.         0.         0.         0.04721601 0.04721601 0.\n",
      "  0.         0.03359458 0.04721601 0.04721601 0.09443201 0.04721601\n",
      "  0.09443201 0.04721601 0.04721601 0.04721601 0.04721601 0.\n",
      "  0.         0.04721601 0.         0.23516204 0.         0.04721601\n",
      "  0.04721601 0.33051204 0.04721601 0.04721601 0.         0.\n",
      "  0.04721601 0.         0.04721601 0.04721601 0.04721601 0.\n",
      "  0.         0.04721601 0.04721601 0.09443201]\n",
      " [0.08865957 0.         0.         0.         0.         0.\n",
      "  0.         0.08865957 0.         0.25232806 0.         0.\n",
      "  0.         0.08865957 0.08865957 0.08865957 0.         0.08865957\n",
      "  0.         0.         0.         0.12616403 0.         0.\n",
      "  0.17731914 0.         0.         0.         0.         0.08865957\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.08865957 0.         0.         0.\n",
      "  0.08865957 0.08865957 0.08865957 0.08865957 0.         0.08865957\n",
      "  0.         0.08865957 0.         0.08865957 0.         0.\n",
      "  0.         0.08865957 0.         0.         0.         0.\n",
      "  0.         0.         0.08865957 0.         0.         0.\n",
      "  0.         0.         0.12616403 0.         0.         0.\n",
      "  0.         0.         0.17731914 0.08865957 0.08865957 0.\n",
      "  0.         0.08865957 0.         0.08865957 0.         0.\n",
      "  0.         0.08865957 0.         0.         0.         0.08865957\n",
      "  0.         0.         0.         0.08865957 0.08865957 0.\n",
      "  0.         0.         0.12616403 0.08865957 0.         0.\n",
      "  0.         0.08865957 0.         0.08865957 0.4415741  0.06308201\n",
      "  0.08865957 0.08865957 0.         0.         0.         0.\n",
      "  0.         0.08865957 0.         0.         0.08865957 0.\n",
      "  0.08865957 0.         0.08865957 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08865957 0.08865957\n",
      "  0.         0.         0.08865957 0.08865957 0.         0.08865957\n",
      "  0.         0.         0.08865957 0.         0.         0.08865957\n",
      "  0.08865957 0.08865957 0.08865957 0.         0.         0.08865957\n",
      "  0.08865957 0.06308201 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.08865957\n",
      "  0.08865957 0.         0.08865957 0.37849209 0.08865957 0.\n",
      "  0.         0.         0.         0.         0.08865957 0.08865957\n",
      "  0.         0.08865957 0.         0.         0.         0.17731914\n",
      "  0.17731914 0.         0.         0.        ]]\n",
      "['acut', 'addit', 'administr', 'after', 'against', 'agent', 'allograft', 'also', 'an', 'and', 'antiapoptot', 'antiinflammatori', 'apoptosi', 'ard', 'area', 'art', 'at', 'attent', 'bacteri', 'be', 'bilirubin', 'by', 'can', 'carbon', 'care', 'catalyz', 'cellular', 'chronic', 'cigarett', 'clinic', 'co', 'concentr', 'confer', 'constitut', 'contribut', 'convers', 'cytoprotect', 'damag', 'debat', 'degrad', 'discuss', 'diseas', 'distress', 'document', 'documnent', 'domin', 'effect', 'emerg', 'emphasi', 'enthusiasm', 'enzymat', 'ethic', 'exogen', 'exposur', 'express', 'extracorpor', 'ferrou', 'for', 'from', 'function', 'growth', 'heme', 'high', 'ho', 'hoco', 'hypoxia', 'implic', 'import', 'in', 'includ', 'induc', 'inflamm', 'inflammatori', 'influenc', 'intens', 'interest', 'intern', 'involv', 'iron', 'is', 'isozym', 'issu', 'it', 'ixα', 'key', 'lectur', 'like', 'limit', 'lipopolysaccharid', 'liver', 'low', 'lowlevel', 'mani', 'mark', 'mart', 'may', 'mechan', 'mediat', 'medicin', 'meet', 'metabolit', 'monoxid', 'most', 'noninvas', 'number', 'occur', 'of', 'on', 'other', 'overal', 'oxid', 'oxygenas', 'oxygennitrogen', 'physiolog', 'play', 'posit', 'prevent', 'process', 'procon', 'product', 'progress', 'proinflammatori', 'promis', 'protect', 'protein', 'provid', 'pulmonari', 'ratelimit', 'reaction', 'reactiv', 'receiv', 'recent', 'reject', 'remark', 'renew', 'replac', 'respect', 'respiratori', 'respond', 'respons', 'result', 'review', 'role', 'second', 'seminar', 'sens', 'sepsi', 'smoke', 'speci', 'st', 'standard', 'state', 'step', 'stimulu', 'stress', 'stude', 'student', 'studi', 'substitut', 'such', 'suggest', 'symposium', 'syndrom', 'system', 'that', 'the', 'therapi', 'thi', 'tissu', 'to', 'transcript', 'transplant', 'trial', 'tutori', 'upregul', 'ventil', 'virtu', 'vitro', 'vivo', 'wa', 'were', 'which', 'will', 'with']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lita4\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def calculateTFIDF(tokenized_docs):\n",
    "    # Initialize the TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in tokenized_docs])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "tfidf_matrix, feature_names  = calculateTFIDF(tokenized_docs)\n",
    "\n",
    "print(tfidf_matrix.toarray())\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5aaaa98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'matrix' and 'coo_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22312\\1956575527.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbm25_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mbm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22312\\1956575527.py\u001b[0m in \u001b[0;36mbm\u001b[1;34m(tokenized_docs)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mk1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mbm25_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midf_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midf_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# print the BM25 weights of the first document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'matrix' and 'coo_matrix'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "def bm(tokenized_docs):\n",
    "    # convert the tokenized documents to a list of strings\n",
    "    docs = [' '.join(tokens) for tokens in tokenized_docs]\n",
    "\n",
    "    # create a bag of words representation of the documents\n",
    "    vectorizer = CountVectorizer()\n",
    "    print(vectorizer)\n",
    "    \n",
    "    bow = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # calculate the inverse document frequency (IDF) weights of the documents\n",
    "    transformer = TfidfTransformer()\n",
    "    idf_weights = transformer.fit_transform(bow)\n",
    "\n",
    "    # calculate the BM25 weights of the documents\n",
    "    b = 0.75\n",
    "    k1 = 1.2\n",
    "    bm25_weights = safe_sparse_dot(idf_weights, vectorizer.transform(docs).T).T * (k1 + 1) / (safe_sparse_dot(idf_weights, vectorizer.transform(docs).T).T + k1 * (1 - b + b * vectorizer.transform(docs).sum(axis=1) / vectorizer.transform(docs).max(axis=1)))\n",
    "\n",
    "    # print the BM25 weights of the first document\n",
    "    print(bm25_weights[0].toarray()[0])\n",
    "    \n",
    "bm(tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78d03c",
   "metadata": {},
   "source": [
    "# Sparse Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27080096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "131a3cc7",
   "metadata": {},
   "source": [
    "# Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567aef90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
