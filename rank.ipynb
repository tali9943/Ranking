{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7d86ce",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbb2d2",
   "metadata": {},
   "source": [
    "Second Assignement of Learning with Massive Data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2729da7c",
   "metadata": {},
   "source": [
    "# Usefull Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "629c8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def score_table(score):\n",
    "    df = pd.DataFrame(score)\n",
    "\n",
    "    df.index = [\"Query \" + str(i+1) for i in range(len(df.index))]\n",
    "    df.columns = [\"Document \" + str(i+1) for i in range(len(df.columns))]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26dd9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "path_name_documents = './Databases/prova/gigi.jsonl'\n",
    "path_name_query = './Databases/prova/query.jsonl'\n",
    "\n",
    "\n",
    "def readFile(path_name):\n",
    "    # Load the JSONL file into a list\n",
    "    with open(path_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Convert each JSON object into a dictionary\n",
    "    dicts = [json.loads(line) for line in lines]\n",
    "\n",
    "    # Convert the dictionaries into arrays and stack them vertically\n",
    "    arrays = np.vstack([np.array(list(d.values())) for d in dicts])\n",
    "\n",
    "    # Convert the arrays into a list of lists\n",
    "    text = arrays.tolist()\n",
    "    \n",
    "    return text\n",
    "\n",
    "documents = readFile(path_name_documents)\n",
    "#print(documents)\n",
    "queries = readFile(path_name_query)\n",
    "#print(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade98943",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2820a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stemmingLemming(filtered_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Perform stemming or lemmatization on filtered tokens\n",
    "    \n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return filtered_tokens\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "def tokenize(path_name):\n",
    "    \n",
    "    with open(path_name, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "        # Create an empty list to store the tokenized documents\n",
    "        tokenized_docs = []\n",
    "\n",
    "        # Loop through each line in the JSONL file\n",
    "        for line in data:\n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            doc = json.loads(line)\n",
    "\n",
    "            # Extract the text from the dictionary\n",
    "            text = doc['text']\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            #text = re.sub(r'\\d+', '', text)  # Remove all numbers\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove all punctuation\n",
    "\n",
    "            # Tokenize the text using NLTK\n",
    "            tokens = word_tokenize(text)\n",
    "            tokensStemLem = stemmingLemming(tokens)\n",
    "\n",
    "            # Add the tokenized document to the list\n",
    "            tokenized_docs.append(tokensStemLem)\n",
    "\n",
    "        # Print the tokenized documents\n",
    "    return tokenized_docs\n",
    "\n",
    "tokenized_docs = tokenize(path_name_documents)\n",
    "\n",
    "\n",
    "tokenized_query = tokenize(path_name_query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00052125",
   "metadata": {},
   "source": [
    "# Sparse Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fac39c",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73a901f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def calculateTFIDF(tokenized_docs):\n",
    "    # Initialize the TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in tokenized_docs])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names,vectorizer\n",
    " \n",
    "    \n",
    "    \n",
    "def calculateTFIDFQuery(tokenized_query,vectorizer):\n",
    "    # Initialize the TfidfVectorizer\n",
    "    #vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.transform([' '.join(doc) for doc in tokenized_query])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names\n",
    "\n",
    "    \n",
    "\n",
    "tfidf_matrix_docs, feature_names_docs,vectorizer  = calculateTFIDF(tokenized_docs)\n",
    "tfidf_matrix_query, feature_names_query  = calculateTFIDFQuery( tokenized_query,vectorizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e1b30",
   "metadata": {},
   "source": [
    "# Dot Product for Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92de26cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Document 1  Document 2  Document 3  Document 4  Document 5  \\\n",
      "Query 1    0.183441    0.339340    0.135609    0.366428    0.377008   \n",
      "Query 2    0.219293    0.149213    0.102671    0.199109    0.225653   \n",
      "Query 3    0.037007    0.015821    0.090983    0.018307    0.094116   \n",
      "Query 4    0.021409    0.000000    0.026377    0.000000    0.016987   \n",
      "Query 5    0.079071    0.015343    0.010202    0.053696    0.049601   \n",
      "Query 6    0.072174    0.062264    0.034060    0.096203    0.083014   \n",
      "Query 7    0.074665    0.016662    0.030792    0.115323    0.130225   \n",
      "Query 8    0.202117    0.220873    0.101320    0.287763    0.256384   \n",
      "\n",
      "         Document 6  Document 7  Document 8  Document 9  Document 10  \\\n",
      "Query 1    0.215299    0.168381    0.218235    0.097616     0.334944   \n",
      "Query 2    0.079196    0.171523    0.147755    0.229581     0.157818   \n",
      "Query 3    0.057963    0.094272    0.006934    0.027960     0.016734   \n",
      "Query 4    0.000000    0.089043    0.052155    0.000000     0.000000   \n",
      "Query 5    0.123762    0.019001    0.037314    0.077973     0.016228   \n",
      "Query 6    0.101917    0.036720    0.044898    0.055754     0.056447   \n",
      "Query 7    0.035374    0.159456    0.093302    0.137506     0.017623   \n",
      "Query 8    0.187501    0.173456    0.141201    0.131109     0.200238   \n",
      "\n",
      "         Document 11  \n",
      "Query 1     0.188142  \n",
      "Query 2     0.224914  \n",
      "Query 3     0.037955  \n",
      "Query 4     0.021957  \n",
      "Query 5     0.062396  \n",
      "Query 6     0.074024  \n",
      "Query 7     0.076579  \n",
      "Query 8     0.207297  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_Dot_Product_Sparse(tfidf_matrix_docs, tfidf_matrix_query):\n",
    "    matrix_docs = tfidf_matrix_docs.toarray()\n",
    "    matrix_query = tfidf_matrix_query.toarray()\n",
    "    num_queries = matrix_query.shape[0]\n",
    "    num_docs = matrix_docs.shape[0]\n",
    "    sparse_score_results = []\n",
    "    \n",
    "    for query in range(num_queries):\n",
    "        sparse_score_docs = []\n",
    "        for doc in range(num_docs):\n",
    "            dot_result = np.dot(matrix_query[query], matrix_docs[doc])\n",
    "            sparse_score_docs.append(dot_result)\n",
    "        sparse_score_results.append(sparse_score_docs)\n",
    "        \n",
    "    return sparse_score_results\n",
    "\n",
    "\n",
    "\n",
    "sparse_score_results = calculate_Dot_Product_Sparse(tfidf_matrix_docs,tfidf_matrix_query)\n",
    "sparse_score_table = score_table(sparse_score_results)\n",
    "print(sparse_score_table)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a3cc7",
   "metadata": {},
   "source": [
    "# Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "567aef90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def create_dense_vector(document):\n",
    "    # Load the all-MiniLM-L6-v2 model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Create a dense vector representation of the document\n",
    "    dense_vector = model.encode(document, convert_to_tensor=True)\n",
    "    \n",
    "    \n",
    "    return dense_vector\n",
    "\n",
    "\n",
    "dense_vector_document = create_dense_vector(documents)\n",
    "dense_vector_query = create_dense_vector(queries)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc5ce1",
   "metadata": {},
   "source": [
    "# Dot Product for Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbb07163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Document 1  Document 2  Document 3  Document 4  Document 5  \\\n",
      "Query 1    0.077958    0.038245    0.133962    0.106983    0.226969   \n",
      "Query 2    0.099590    0.070468    0.066888    0.096966    0.279706   \n",
      "Query 3    0.115690    0.103854    0.194563    0.120762    0.322730   \n",
      "Query 4    0.144724    0.175452    0.131543    0.226517    0.249557   \n",
      "Query 5    0.229337    0.190014    0.095667    0.222562    0.330017   \n",
      "Query 6    0.092435    0.147272    0.173545    0.096595    0.171428   \n",
      "Query 7    0.174007    0.177980    0.213392    0.104680    0.231447   \n",
      "Query 8    0.129492    0.135007    0.152630    0.118768    0.170204   \n",
      "\n",
      "         Document 6  Document 7  Document 8  Document 9  Document 10  \\\n",
      "Query 1    0.126819    0.324864    0.217792    0.038656     0.038245   \n",
      "Query 2    0.121130    0.310443    0.200889    0.052011     0.070468   \n",
      "Query 3    0.129222    0.261768    0.200194    0.167548     0.103854   \n",
      "Query 4    0.214995    0.339539    0.206882    0.168591     0.175452   \n",
      "Query 5    0.203857    0.242839    0.142185    0.110872     0.190014   \n",
      "Query 6    0.057788    0.219801    0.182564    0.211420     0.147272   \n",
      "Query 7    0.153244    0.285167    0.150482    0.233630     0.177980   \n",
      "Query 8    0.102133    0.181831    0.162573    0.278159     0.135007   \n",
      "\n",
      "         Document 11  \n",
      "Query 1     0.077958  \n",
      "Query 2     0.099590  \n",
      "Query 3     0.115690  \n",
      "Query 4     0.144724  \n",
      "Query 5     0.229337  \n",
      "Query 6     0.092435  \n",
      "Query 7     0.174007  \n",
      "Query 8     0.129492  \n"
     ]
    }
   ],
   "source": [
    "def calculate_Dot_Product_Dense(dense_vector_document, dense_vector_query):\n",
    "    num_queries = dense_vector_query.shape[0]\n",
    "    num_docs = dense_vector_document.shape[0]\n",
    "    dense_score_results = []\n",
    "    \n",
    "    for query in range(num_queries):\n",
    "        dense_score_docs = []\n",
    "        for doc in range(num_docs):\n",
    "            dot_result = np.dot(dense_vector_query[query], dense_vector_document[doc])\n",
    "            dense_score_docs.append(dot_result)\n",
    "        dense_score_results.append(dense_score_docs)\n",
    "        \n",
    "    return dense_score_results\n",
    "\n",
    "\n",
    "dense_score_results = calculate_Dot_Product_Dense(dense_vector_document, dense_vector_query)\n",
    "dense_score_table = score_table(dense_score_results)\n",
    "print(dense_score_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ed758",
   "metadata": {},
   "source": [
    "# Calculate the final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2c6134",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Document 1  Document 2  Document 3  Document 4  Document 5  \\\n",
      "Query 1    0.261399    0.377585    0.269571    0.473410    0.603978   \n",
      "Query 2    0.318883    0.219681    0.169559    0.296075    0.505359   \n",
      "Query 3    0.152697    0.119676    0.285546    0.139068    0.416846   \n",
      "Query 4    0.166132    0.175452    0.157920    0.226517    0.266544   \n",
      "Query 5    0.308408    0.205356    0.105869    0.276258    0.379617   \n",
      "Query 6    0.164609    0.209536    0.207605    0.192798    0.254442   \n",
      "Query 7    0.248673    0.194642    0.244185    0.220003    0.361672   \n",
      "Query 8    0.331609    0.355880    0.253950    0.406531    0.426588   \n",
      "\n",
      "         Document 6  Document 7  Document 8  Document 9  Document 10  \\\n",
      "Query 1    0.342117    0.493245    0.436028    0.136272     0.373188   \n",
      "Query 2    0.200326    0.481966    0.348644    0.281591     0.228287   \n",
      "Query 3    0.187185    0.356040    0.207127    0.195508     0.120588   \n",
      "Query 4    0.214995    0.428582    0.259037    0.168591     0.175452   \n",
      "Query 5    0.327619    0.261840    0.179499    0.188846     0.206241   \n",
      "Query 6    0.159705    0.256521    0.227462    0.267174     0.203719   \n",
      "Query 7    0.188618    0.444623    0.243784    0.371136     0.195603   \n",
      "Query 8    0.289633    0.355287    0.303773    0.409268     0.335245   \n",
      "\n",
      "         Document 11  \n",
      "Query 1     0.266100  \n",
      "Query 2     0.324503  \n",
      "Query 3     0.153645  \n",
      "Query 4     0.166681  \n",
      "Query 5     0.291733  \n",
      "Query 6     0.166459  \n",
      "Query 7     0.250586  \n",
      "Query 8     0.336789  \n"
     ]
    }
   ],
   "source": [
    "def sum_vectors_score(dense_score, sparse_score):\n",
    "    final_score = []\n",
    "    for i in range(len(dense_score)):\n",
    "        row = []\n",
    "        for j in range(len(dense_score[i])):\n",
    "            row.append(dense_score[i][j] + sparse_score[i][j])\n",
    "        final_score.append(row)\n",
    "    return final_score\n",
    "\n",
    "final_score = sum_vectors_score(dense_score_results,sparse_score_results)\n",
    "\n",
    "    \n",
    "def table_final_score(final_score):\n",
    "    df = pd.DataFrame(final_score)\n",
    "\n",
    "    df.index = [\"Query \" + str(i+1) for i in range(len(df.index))]\n",
    "    df.columns = [\"Document \" + str(i+1) for i in range(len(df.columns))]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = table_final_score(final_score)\n",
    "print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f3f99",
   "metadata": {},
   "source": [
    "# Tok-k documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a621d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Document 1                Document 2  \\\n",
      "Query 1   (4, 0.6039777237570092)   (6, 0.4932448673431095)   \n",
      "Query 2   (4, 0.5053588847080347)  (6, 0.48196612801000327)   \n",
      "Query 3   (4, 0.4168460690526282)  (6, 0.35603950481593094)   \n",
      "Query 4  (6, 0.42858238921732694)  (4, 0.26654419827789694)   \n",
      "Query 5   (4, 0.3796173551092559)   (5, 0.3276194630114711)   \n",
      "Query 6  (8, 0.26717389429683513)  (6, 0.25652085826633153)   \n",
      "Query 7   (6, 0.4446225101574543)  (8, 0.37113577447895024)   \n",
      "Query 8   (4, 0.4265884576560837)   (8, 0.4092677309067976)   \n",
      "\n",
      "                       Document 3  \n",
      "Query 1   (3, 0.4734102674770713)  \n",
      "Query 2   (7, 0.3486439400161163)  \n",
      "Query 3  (2, 0.28554640233045037)  \n",
      "Query 4  (7, 0.25903672514783505)  \n",
      "Query 5   (0, 0.3084077697406177)  \n",
      "Query 6   (4, 0.2544423221988228)  \n",
      "Query 7   (4, 0.3616721029467196)  \n",
      "Query 8  (3, 0.40653094177607463)  \n"
     ]
    }
   ],
   "source": [
    "def found_top_k(final_score, k):\n",
    "    top_k = []\n",
    "    for scores in final_score:\n",
    "        sorted_scores = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:k]\n",
    "        top_k.append([(index, score) for index, score in sorted_scores])\n",
    "    return top_k\n",
    "\n",
    "\n",
    "list_top_k = found_top_k(final_score,3) \n",
    "\n",
    "results_k = table_final_score(list_top_k)\n",
    "print(results_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1235d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
