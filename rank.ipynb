{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7d86ce",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbb2d2",
   "metadata": {},
   "source": [
    "Second Assignement of Learning with Massive Data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26dd9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "path_name_documents = './Databases/prova/gigi.jsonl'\n",
    "path_name_query = './Databases/prova/query.jsonl'\n",
    "\n",
    "\n",
    "def readFile(path_name):\n",
    "    # Load the JSONL file into a list\n",
    "    with open(path_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Convert each JSON object into a dictionary\n",
    "    dicts = [json.loads(line) for line in lines]\n",
    "\n",
    "    # Convert the dictionaries into arrays and stack them vertically\n",
    "    arrays = np.vstack([np.array(list(d.values())) for d in dicts])\n",
    "\n",
    "    # Convert the arrays into a list of lists\n",
    "    text = arrays.tolist()\n",
    "    \n",
    "    return text\n",
    "\n",
    "documents = readFile(path_name_documents)\n",
    "#print(documents)\n",
    "query = readFile(path_name_query)\n",
    "#print(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade98943",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2820a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stemmingLemming(filtered_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Perform stemming or lemmatization on filtered tokens\n",
    "    \n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return filtered_tokens\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "def tokenize(path_name):\n",
    "    \n",
    "    with open(path_name, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "        # Create an empty list to store the tokenized documents\n",
    "        tokenized_docs = []\n",
    "\n",
    "        # Loop through each line in the JSONL file\n",
    "        for line in data:\n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            doc = json.loads(line)\n",
    "\n",
    "            # Extract the text from the dictionary\n",
    "            text = doc['text']\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            #text = re.sub(r'\\d+', '', text)  # Remove all numbers\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove all punctuation\n",
    "\n",
    "            # Tokenize the text using NLTK\n",
    "            tokens = word_tokenize(text)\n",
    "            tokensStemLem = stemmingLemming(tokens)\n",
    "\n",
    "            # Add the tokenized document to the list\n",
    "            tokenized_docs.append(tokensStemLem)\n",
    "\n",
    "        # Print the tokenized documents\n",
    "    return tokenized_docs\n",
    "\n",
    "tokenized_docs = tokenize(path_name_documents)\n",
    "\n",
    "\n",
    "tokenized_query = tokenize(path_name_query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72f5c9",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8650446d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "\n",
    "def calculateTF(tokenized_docs):\n",
    "    term_freqs = []\n",
    "    for doc in tokenized_docs:\n",
    "        doc_freq = collections.Counter(doc) #number of repetition for each word\n",
    "       \n",
    "        total_terms = len(doc) #length for each document\n",
    "        \n",
    "        for term in doc_freq:\n",
    "            doc_freq[term] /= float(total_terms)\n",
    "        term_freqs.append(doc_freq)\n",
    "\n",
    "    return term_freqs\n",
    "\n",
    "\n",
    "\n",
    "def calculateTF2(tokenized_docs):\n",
    "    term_freqs = []\n",
    "    for doc in tokenized_docs:\n",
    "        doc_freq = {}\n",
    "        total_terms = len(doc)\n",
    "        for term in doc:\n",
    "            doc_freq[term] = doc_freq.get(term, 0) + 1\n",
    "        for term in doc_freq:\n",
    "            doc_freq[term] /= float(total_terms)\n",
    "        term_freqs.append(doc_freq)\n",
    "\n",
    "    return term_freqs\n",
    "\n",
    "\n",
    "\n",
    "def calculateTFbm25(tokenized_docs):\n",
    "    term_freqs = []\n",
    "    k1 = 1.5 # parameter for controlling term frequency normalization\n",
    "    b = 0.75 # parameter for controlling document length normalization\n",
    "    avgdl = sum(len(doc) for doc in tokenized_docs) / len(tokenized_docs) # average document length\n",
    "\n",
    "    for doc in tokenized_docs:\n",
    "        doc_freq = collections.Counter(doc) #number of repetition for each word\n",
    "        total_terms = len(doc) #length for each document\n",
    "        doc_len_norm = ((1 - b) + b * (total_terms / avgdl)) # document length normalization factor\n",
    "\n",
    "        for term in doc_freq:\n",
    "            tf = doc_freq[term] / total_terms # term frequency\n",
    "            tf_norm = ((k1 + 1) * tf) / (k1 * ((1 - b) + b * (total_terms / avgdl)) + tf) # normalized term frequency with BM25 weighting\n",
    "            doc_freq[term] = tf_norm\n",
    "\n",
    "        term_freqs.append(doc_freq)\n",
    "\n",
    "    return term_freqs\n",
    "\n",
    "    \n",
    "#term_freqs = calculateTF2(tokenized_docs)\n",
    "term_freqs = calculateTFbm25(tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76afbcbc",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "504063d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculateIDF(tokenized_docs,term_freqs):\n",
    "    \n",
    "    #dictionary with the frequency of each term in all documents\n",
    "    all_terms = [term for doc in tokenized_docs for term in doc]\n",
    "    \n",
    "    df = collections.Counter(all_terms)\n",
    "\n",
    "    # Calculate IDF for each term\n",
    "    N = len(tokenized_docs)\n",
    "    \n",
    "    idf = {}\n",
    "    \n",
    "    for term in df:\n",
    "        n = len([doc for doc in tokenized_docs if term in doc])\n",
    "        idf[term] = math.log(N / float(df[term] + 1)) #math.log(1 + (N - n + 0.5)/(n + 0.5))\n",
    "    return idf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculateIDFBM25(tokenized_docs):\n",
    "    #dictionary with the frequency of each term in all documents\n",
    "    all_terms = [term for doc in tokenized_docs for term in doc]\n",
    "    \n",
    "    df = defaultdict(int)\n",
    "\n",
    "    # Calculate DF for each term\n",
    "    for term in all_terms:\n",
    "        df[term] += 1\n",
    "    # Calculate IDF for each term\n",
    "    N = len(tokenized_docs)\n",
    "    \n",
    "    idf = {}\n",
    "    \n",
    "    for term in df:\n",
    "        n = len([doc for doc in tokenized_docs if term in doc])\n",
    "        idf[term] = math.log((N - n + 0.5)/(n + 0.5)+1)\n",
    "    \n",
    "    return idf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#idf = calculateIDFBM25(tokenized_docs)\n",
    "idf = calculateIDF(tokenized_docs,term_freqs)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00052125",
   "metadata": {},
   "source": [
    "# Step successivi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fac39c",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a901f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF- DOCUMENTS:\n",
      "  (0, 153)\t0.03366972268896855\n",
      "  (0, 74)\t0.0473216204390231\n",
      "  (0, 109)\t0.03366972268896855\n",
      "  (0, 50)\t0.0473216204390231\n",
      "  (0, 144)\t0.0473216204390231\n",
      "  (0, 169)\t0.0473216204390231\n",
      "  (0, 42)\t0.0473216204390231\n",
      "  (0, 21)\t0.0473216204390231\n",
      "  (0, 184)\t0.0473216204390231\n",
      "  (0, 43)\t0.0473216204390231\n",
      "  (0, 165)\t0.0473216204390231\n",
      "  (0, 66)\t0.0473216204390231\n",
      "  (0, 68)\t0.0473216204390231\n",
      "  (0, 100)\t0.03366972268896855\n",
      "  (0, 129)\t0.0946432408780462\n",
      "  (0, 69)\t0.0473216204390231\n",
      "  (0, 36)\t0.0473216204390231\n",
      "  (0, 150)\t0.0473216204390231\n",
      "  (0, 30)\t0.0473216204390231\n",
      "  (0, 60)\t0.0473216204390231\n",
      "  (0, 55)\t0.0473216204390231\n",
      "  (0, 54)\t0.0473216204390231\n",
      "  (0, 93)\t0.0473216204390231\n",
      "  (0, 29)\t0.0473216204390231\n",
      "  (0, 61)\t0.0473216204390231\n",
      "  :\t:\n",
      "  (1, 174)\t0.0883131559753662\n",
      "  (1, 31)\t0.0883131559753662\n",
      "  (1, 134)\t0.0883131559753662\n",
      "  (1, 143)\t0.0883131559753662\n",
      "  (1, 47)\t0.0883131559753662\n",
      "  (1, 181)\t0.1766263119507324\n",
      "  (1, 49)\t0.0883131559753662\n",
      "  (1, 26)\t0.1766263119507324\n",
      "  (1, 76)\t0.1766263119507324\n",
      "  (1, 163)\t0.0883131559753662\n",
      "  (1, 78)\t0.0883131559753662\n",
      "  (1, 1)\t0.0883131559753662\n",
      "  (1, 96)\t0.0883131559753662\n",
      "  (1, 0)\t0.0883131559753662\n",
      "  (1, 46)\t0.0883131559753662\n",
      "  (1, 45)\t0.0883131559753662\n",
      "  (1, 146)\t0.0883131559753662\n",
      "  (1, 153)\t0.0628355378343335\n",
      "  (1, 109)\t0.0628355378343335\n",
      "  (1, 100)\t0.125671075668667\n",
      "  (1, 167)\t0.37701322700600104\n",
      "  (1, 23)\t0.125671075668667\n",
      "  (1, 108)\t0.4398487648403345\n",
      "  (1, 11)\t0.251342151337334\n",
      "  (1, 70)\t0.125671075668667\n",
      "TFIDF- QUERY:\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.57735027 0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def calculateTFIDF(tokenized_docs):\n",
    "    # Initialize the TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in tokenized_docs])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names,vectorizer\n",
    " \n",
    "    \n",
    "def calculateTFIDFQ(tokenized_query,vectorizer):\n",
    "    # Initialize the TfidfVectorizer\n",
    "    #vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.transform([' '.join(doc) for doc in tokenized_query])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names\n",
    "\n",
    "    \n",
    "\n",
    "tfidf_matrix_docs, feature_names_docs,vectorizer  = calculateTFIDF(tokenized_docs)\n",
    "tfidf_matrix_query, feature_names_query  = calculateTFIDFQ( tokenized_query,vectorizer)\n",
    "\n",
    "print(\"TFIDF- DOCUMENTS:\")\n",
    "#print(tfidf_matrix_docs.toarray())\n",
    "print(tfidf_matrix_docs)\n",
    "\n",
    "print(\"TFIDF- QUERY:\")\n",
    "print(tfidf_matrix_query.toarray())\n",
    "#print(tfidf_matrix_query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e1b30",
   "metadata": {},
   "source": [
    "# Dot Product for Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "92de26cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 0\n",
      "['0.25330518925234474' '0.47194902090194']\n",
      "\n",
      "query: 1\n",
      "['0.4366901266404883' '0.29403954155250905']\n",
      "\n",
      "query: 2\n",
      "['0.05464230059791856' '0.050987624375363076']\n",
      "\n",
      "query: 3\n",
      "['0.0473216204390231' '0.0']\n",
      "\n",
      "query: 4\n",
      "['0.19597462672499155' '0.056481157909239896']\n",
      "\n",
      "query: 5\n",
      "['0.2332726692999068' '0.25499733860601864']\n",
      "\n",
      "query: 6\n",
      "['0.23423007096418416' '0.06244683145816671']\n",
      "\n",
      "query: 7\n",
      "['0.38444687571856695' '0.3983050336478946']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_Dot_Product(tfidf_matrix_docs, tfidf_matrix_query):\n",
    "    matrix_docs = tfidf_matrix_docs.toarray()\n",
    "    matrix_query = tfidf_matrix_query.toarray()\n",
    "    num_queries = matrix_query.shape[0]\n",
    "    num_docs = matrix_docs.shape[0]\n",
    "    sparse_score_results = np.empty(num_queries, dtype=np.ndarray)\n",
    "    \n",
    "    for query in range(num_queries):\n",
    "        sparse_score_docs = np.empty(num_docs)\n",
    "        for doc in range(num_docs):\n",
    "            dot_result = np.dot(matrix_query[query], matrix_docs[doc])\n",
    "            sparse_score_docs[doc] = dot_result\n",
    "        message = f\"query: {query}\"\n",
    "        sparse_score_results[query] = np.array([message] + sparse_score_docs.tolist())\n",
    "        \n",
    "    for i in range(num_queries):\n",
    "        print(sparse_score_results[i][0])\n",
    "        print(sparse_score_results[i][1:])\n",
    "        print()\n",
    "        \n",
    "    return sparse_score_results\n",
    "\n",
    "\n",
    "\n",
    "dot_result = calculate_Dot_Product(tfidf_matrix_docs,tfidf_matrix_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb8d10b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES SPARSE VECTORS\n",
      "      Query     Doc 1     Doc 2\n",
      "0  query: 0  0.253305  0.471949\n",
      "1  query: 1   0.43669   0.29404\n",
      "2  query: 2  0.054642  0.050988\n",
      "3  query: 3  0.047322       0.0\n",
      "4  query: 4  0.195975  0.056481\n",
      "5  query: 5  0.233273  0.254997\n",
      "6  query: 6   0.23423  0.062447\n",
      "7  query: 7  0.384447  0.398305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lita4\\AppData\\Local\\Temp\\ipykernel_10160\\885350967.py:9: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sparse_score_results = np.empty((num_queries, num_docs+1), dtype=np.object)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def tabella(tfidf_matrix_docs, tfidf_matrix_query):\n",
    "    matrix_docs = tfidf_matrix_docs.toarray()\n",
    "    matrix_query = tfidf_matrix_query.toarray()\n",
    "    num_queries = matrix_query.shape[0]\n",
    "    num_docs = matrix_docs.shape[0]\n",
    "    sparse_score_results = np.empty((num_queries, num_docs+1), dtype=np.object)\n",
    "    \n",
    "    for query in range(num_queries):\n",
    "        sparse_score_docs = np.empty(num_docs)\n",
    "        for doc in range(num_docs):\n",
    "            dot_result = np.dot(matrix_query[query], matrix_docs[doc])\n",
    "            sparse_score_docs[doc] = dot_result\n",
    "        message = f\"query: {query}\"\n",
    "        sparse_score_results[query, 0] = message\n",
    "        sparse_score_results[query, 1:] = sparse_score_docs\n",
    "    \n",
    "    df = pd.DataFrame(sparse_score_results, columns=[\"Query\"] + [\"Doc \" + str(i+1) for i in range(num_docs)])\n",
    "    return df\n",
    "\n",
    "df = tabella(tfidf_matrix_docs, tfidf_matrix_query)\n",
    "print(\"SCORES SPARSE VECTORS\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78d03c",
   "metadata": {},
   "source": [
    "# Sparse Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27080096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "131a3cc7",
   "metadata": {},
   "source": [
    "# Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "567aef90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "microsoft/minilm-l6-hybrid-512 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/microsoft/minilm-l6-hybrid-512/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1165\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[0;32m   1167\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1506\u001b[0m     )\n\u001b[1;32m-> 1507\u001b[1;33m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    290\u001b[0m             )\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6435421e-1bb4c6ad51aa7bd92a8a6462)\n\nRepository Not Found for url: https://huggingface.co/microsoft/minilm-l6-hybrid-512/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18036\\2959885414.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load tokenizer and model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"microsoft/minilm-l6-hybrid-512\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"microsoft/minilm-l6-hybrid-512\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[1;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[0mtokenizer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"_commit_hash\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m     ```\"\"\"\n\u001b[0;32m    462\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m     resolved_config_file = cached_file(\n\u001b[0m\u001b[0;32m    464\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         raise EnvironmentError(\n\u001b[0m\u001b[0;32m    425\u001b[0m             \u001b[1;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[1;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: microsoft/minilm-l6-hybrid-512 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Step 1: Load the MiniLM-L6-v2 model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"allenai/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# Step 2: Convert the text into tokens using the tokenizer\n",
    "text = \"This is an example sentence.\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Step 3: Pass the encoded input through the MiniLM-L6-v2 model\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Step 4: Extract the dense vector from the model output\n",
    "dense_vector = model_output.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "print(dense_vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc5ce1",
   "metadata": {},
   "source": [
    "# INSERISCI ARGOMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb07163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be118f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
